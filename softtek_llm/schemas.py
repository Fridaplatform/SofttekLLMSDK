from typing import Dict, List, Literal, Optional

from pydantic import BaseModel


class Message(BaseModel):
    """
    # Message
    Model class to represent a message

    ## Attributes
    - `role` (Literal["system", "user", "assistant", "function"]): the role of the message
    - `content` (str): the content of the message
    """

    role: Literal["system", "user", "assistant", "function"]
    """The role of the message"""
    content: str
    """The content of the message"""


class Usage(BaseModel):
    """
    # Usage
    Defines Usage class with the following attributes:

    ## Attributes
    - `prompt_tokens`: an integer representing the number of tokens in the prompt
    - `completion_tokens`: an integer representing the number of tokens in the completion
    - `total_tokens`: an integer representing the total number of tokens in the Usage
    """

    prompt_tokens: int = 0
    """The number of tokens in the prompt"""
    completion_tokens: int = 0
    """The number of tokens in the completion"""
    total_tokens: int = 0
    """The total number of tokens in the Usage"""


class Response(BaseModel):
    """
    # Response
    Represents the response from an inference API.

    ## Attributes
    - `message` (Message): Message object that the API generated as a response.
    - `created` (int): Unix timestamp for when the response was created.
    - `latency` (int): Time in milliseconds taken to generate the response.
    - `from_cache` (bool): Whether the response was served from cache or not.

    ## Optional Attributes
    - `model` (str): String representation of the model used to generate the response. Defaults to "".
    - `usage` (Usage): Usage object containing metrics of resource usage from generating the response. Defaults to Usage().
    - `additional_kwargs` (Dict): Optional dictionary of additional keyword arguments. Defaults to {}.
    """

    message: Message
    """Message object that the API generated as a response"""
    created: int
    """Unix timestamp for when the response was created"""
    latency: int
    """Time in milliseconds taken to generate the response"""
    from_cache: bool
    """Whether the response was served from cache or not"""

    model: Optional[str] = ""
    """String representation of the model used to generate the response"""
    usage: Optional[Usage] = Usage()
    """Usage object containing metrics of resource usage from generating the response"""
    additional_kwargs: Optional[Dict] = {}
    """Optional dictionary of additional keyword arguments"""


class Filter(BaseModel):
    """
    # Filter
    A model class for a filter object.

    ## Attributes
    - `type` (Literal["ALLOW","DENY"]): The type of the filter instance: ALLOW or DENY
    - `case` (str): The case for which the filter applies
    """

    type: Literal["ALLOW", "DENY"]
    """The type of the filter instance: ALLOW or DENY"""
    case: str
    """The case for which the filter applies"""


class OpenAIChatChoice(BaseModel):
    """
    # OpenAI Chat Choice
    A model class for a choice in an OpenAI chat response.

    ## Attributes
    - `index` (int): The index of the choice in the list of choices.
    - `message` (Message): A chat completion message generated by the model.
    - `finish_reason` (str): The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `function_call` if the model called a function.
    """

    index: int
    """The index of the choice in the list of choices."""
    message: Message
    """A chat completion message generated by the model."""
    finish_reason: str
    """The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `function_call` if the model called a function."""


class OpenAIChatResponse(BaseModel):
    """
    # OpenAI Chat Response
    A model class for an OpenAI chat response.

    ## Attributes
    - `id` (str): A unique identifier for the chat completion.
    - `object` (Literal["chat.completion"]): The object type, which is always `chat.completion`.
    - `created` (int): The Unix timestamp (in seconds) of when the chat completion was created.
    - `model` (str): The model used for the chat completion.
    - `choices` (List[OpenAIChatChoice]): A list of chat completion choices. Can be more than one if n is greater than 1.
    - `usage` (Usage): Usage statistics for the completion request.
    """

    id: str
    """A unique identifier for the chat completion."""
    object: Literal["chat.completion"]
    """The object type, which is always `chat.completion`."""
    created: int
    """The Unix timestamp (in seconds) of when the chat completion was created."""
    model: str
    """The model used for the chat completion."""
    choices: List[OpenAIChatChoice]
    """A list of chat completion choices. Can be more than one if n is greater than 1."""
    usage: Usage
    """Usage statistics for the completion request."""


class Vector(BaseModel):
    """
    # Vector
    A model class for a vector object.

    ## Attributes
    - `embeddings` (List[float]): A list of floating point numbers representing the vector.

    ## Optional Attributes
    - `id` (str): A unique identifier for the vector. Defaults to "".
    - `metadata` (Dict): Optional dictionary of metadata for the vector. Defaults to {}.
    """

    embeddings: List[float]
    """A list of floating point numbers representing the vector."""

    id: Optional[str] = ""
    """A unique identifier for the vector."""
    metadata: Optional[Dict] = {}
    """Optional dictionary of metadata for the vector."""
